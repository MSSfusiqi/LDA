{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "Latent Dirichlet Allocation (LDA) is generative probabilitistic model dealing with collections of data such as corpus. Based on the assumption of `bag of word` and exchangeability, each document in corpus is modeled as random mixture over latent topics and each topic is modeled by a distribution over words. Document is represented in a form of topic probability. In this project, we foucs on the text data, and in this case, each document is represented as a topic probability. We implement variational inference and `EM` algorithm to estimate parameters and performed ** optimazation method to make our algorithm more efficient. We compare our algorithm with other existing `lda` packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Background\n",
    "In our project, we use the the paper \"Latent Dirichlet Allocation\" by David M. Blei, Andrew Y. Ng and Michael I.Jordan. Latent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus, and its basic idea is that each document are represented as random mixtures over latent topics, and each topic is characterized by a distribution over words. In general, LDA follows the following steps:\n",
    "1. Choose $N \\sim Possion(\\xi)$, which represents the document length.\n",
    "2. Choose $\\theta \\sim Dir(\\alpha)$, which $\\theta$ is a column vector representing the topic probability.\n",
    "3. For each of the N words:\n",
    "    - Choose $z_n\\sim Multinomial(\\theta)$, which represents current topic.\n",
    "    - Choose $w_n$ based on $p( w_n \\mid z_n; \\beta)$\n",
    "   \n",
    " $\\beta$ is a L $\\times$ V matrix, $\\beta_{ij} = P( w^j = 1 \\mid z^i = 1)$, which means $\\beta$ represents the probability of generating one particular word given the particular topic.\n",
    "\n",
    "\n",
    "The general probability model for LDA is as follow:\n",
    "$$p(\\theta,z,w \\mid \\alpha,\\beta) = p(\\theta \\mid \\alpha)(\\prod_{n = 1}^{N}\\sum_{z_n}p(z_n \\mid \\theta)p(w_n \\mid z_n,\\beta)) d\\theta$$\n",
    "\n",
    "The probability of the a corpus can be obtained by taking product of the marginal probabilities of single document.\n",
    "$$P(D \\mid \\alpha, \\beta) = \\prod_{d = 1}^{M} \\int p(\\theta_d \\mid \\alpha)(\\prod_{n = 1}^{N_d}\\sum_{z_{dn}}p(z_{dn}\\mid \\theta_d)  p(w_{dn} \\mid z_{dn},\\beta)) d \\theta_d $$\n",
    "In which $D$\n",
    "represents a corpus, and M represents the number of document in a corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a three level hierarchical Bayesian model, so it is more elaborate than some other latent models such as Mixture if unigrams, and pLSA.  \n",
    "- In the Mixture of unigrams, the word distributions can be viewed as representations od topics under the assumption that each document exihibit only one topic. However, LDA allows documents to exihibit multiple topics to different probabilities. \n",
    "\n",
    "- The pLSA model does solves the problem of Mixture of unigrams, but it has further problems that it is not well-defined generative model of documents,which means that it cannot be used to assign probability to a previously unused document. Also, since the linear growth in parameter of the pLSA model, it can causes overfitting. However, LDA sufferes neigher of those problems.\n",
    "\n",
    "\n",
    "From Mixture of unigrams to PLSA TO lDA, the text modeling is improved step by step. LDA introduces the Dirichlet Distribution in the document to topic layer, which is better than PLSA, so that the number of model parameters does not expand with the increase of corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describtion of Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
