{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siqifu/anaconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "[nltk_data] Downloading package wordnet to /Users/siqifu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('articles2.csv').head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df[['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siqifu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_text['index'] = df_text.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [patriot, peter, berg, thriller, recreat, bost...\n",
       "1    [nors, mytholog, human, world, creat, pantheon...\n",
       "2    [democraci, work, increasingli, divers, nation...\n",
       "3    [updat, januari, press, confer, juli, donald, ...\n",
       "4    [updat, month, equivoc, origin, cyberattack, t...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['content'].map(preprocess)\n",
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 accur\n",
      "1 action\n",
      "2 aftermath\n",
      "3 agent\n",
      "4 agon\n",
      "5 alexand\n",
      "6 ambigu\n",
      "7 analysi\n",
      "8 angri\n",
      "9 ask\n",
      "10 attack\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    '''\n",
    "    Lenmmatize and stem the text.\n",
    "    '''\n",
    "    return PorterStemmer().stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    '''\n",
    "    Preprocess the text.\n",
    "    '''\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "processed_docs = documents['content'].map(preprocess)\n",
    "\n",
    "vocabulary = gensim.corpora.Dictionary(processed_docs)\n",
    "#vocabulary.filter_extremes(no_below=5, no_above=0.1, keep_n=100)\n",
    "bow_corpus = [vocabulary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "doc = [dict(bow) for bow in bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "from scipy.special import digamma, polygamma\n",
    "from numba import jit, float64, int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_test1:\n",
    "    \n",
    "    @staticmethod\n",
    "    def _convergence_(new, old, epsilon = 1.0e-3):\n",
    "        '''\n",
    "        Check convergence.\n",
    "        '''\n",
    "        return np.all(np.abs(new - old)) < epsilon\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _normalization(x):\n",
    "        '''\n",
    "        Normalize the input.\n",
    "        '''\n",
    "        return x/np.sum(x)\n",
    "    \n",
    "  \n",
    "    @staticmethod\n",
    "    def _normalization_row(x):\n",
    "        return x/np.sum(x,1)[:,None]\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def _normalization_col(x):\n",
    "        '''\n",
    "        Normalize a matrix. \n",
    "        Each element is divided by the corresponding column sum.\n",
    "        '''\n",
    "        return x/np.sum(x,0)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    @numba.jit(cache = True)\n",
    "    def _accumulate_Phi(beta, Phi, doc):\n",
    "        '''\n",
    "        This function accumulates the effect of Phi_new from all documents after e step.\n",
    "        beta is V*k matrix.\n",
    "        Phi is N_d * k matrix.\n",
    "        Return updated beta.\n",
    "        '''\n",
    "        \n",
    "        beta[list(doc.keys()),:] += np.diag(list(doc.values())) @ Phi\n",
    "    \n",
    "        return beta\n",
    "    \n",
    "    \n",
    "    def __init__(self, k, max_em_iter=10, max_alpha_iter=10, max_Estep_iter=10):\n",
    "        self._k = k\n",
    "        self._max_em_iter = max_em_iter\n",
    "        self._max_alpha_iter = max_alpha_iter\n",
    "        self._max_Estep_iter = max_Estep_iter\n",
    "        \n",
    "     \n",
    "     \n",
    "    def initializaiton(self, V):\n",
    "        '''\n",
    "        Initialize alpha and beta. \n",
    "        alpha is a k-dim vector. beta is V*k matrix.\n",
    "        '''\n",
    "        \n",
    "        k = self._k\n",
    "        np.random.seed(12345)\n",
    "        alpha = self._normalization(np.random.uniform(size = k))\n",
    "    \n",
    "        beta = np.random.dirichlet(alpha, V)\n",
    "        \n",
    "        return alpha, beta\n",
    "    \n",
    "    \n",
    "    def Estep(self, doc, alpha, beta, N_d):\n",
    "        '''\n",
    "        E step for a document, which calculate the posterior parameters.\n",
    "        beta_old and alpha-old is coming from previous iteration.\n",
    "        Return Phi and gamma  of a document.\n",
    "        '''\n",
    "        \n",
    "        k = self._k\n",
    "        max_iter = self._max_Estep_iter\n",
    "        \n",
    "        gamma_old = alpha + np.ones(k) * N_d/k\n",
    "        row_index = list(doc.keys())\n",
    "        word_count = np.array(list(doc.values()))\n",
    "    \n",
    "        for i in range(max_iter):\n",
    "            # Update Phi\n",
    "            Phi_exp = np.exp(digamma(gamma_old))\n",
    "            Phi = beta[row_index,:] @ np.diag(Phi_exp)\n",
    "            Phi_new = self._normalization_row(Phi)\n",
    "        \n",
    "            # Update gamma\n",
    "            Phi_sum = Phi_new.T @ word_count[:,None] # k-dim\n",
    "            gamma_new = alpha + Phi_sum.T[0]\n",
    "        \n",
    "            # Converge or not\n",
    "            if (i>0) & self._convergence_(gamma_new, gamma_old):\n",
    "                break\n",
    "            else:\n",
    "                gamma_old = gamma_new.copy()\n",
    "    \n",
    "            \n",
    "        return gamma_new, Phi_new\n",
    "    \n",
    "\n",
    "    def newton_raphson(self, alpha_old, gamma_matrix):\n",
    "        '''\n",
    "        This function uses New Raphson method to update alpha in the M step.\n",
    "        alpha_old is a k-dim vector.\n",
    "        gamma_matrix is a M * k matrix which stores all gamma from M documents.\n",
    "        Return updated alpha.\n",
    "        '''\n",
    "\n",
    "        k = self._k\n",
    "        max_iter = self._max_alpha_iter\n",
    "        \n",
    "        M = gamma_matrix.shape[0]\n",
    "        pg = np.sum(digamma(gamma_matrix), 0) - np.sum(digamma(np.sum(gamma_matrix, 1)))\n",
    "        alpha_new = alpha_old.copy()\n",
    "    \n",
    "        for t in range(max_iter):\n",
    "        \n",
    "            alpha_sum = np.sum(alpha_old)\n",
    "            g = M * (digamma(alpha_sum) - digamma(alpha_old)) + pg\n",
    "            h = -M * polygamma(1, alpha_old)\n",
    "            z = M * polygamma(1, alpha_sum)\n",
    "            c = np.sum(g/h)/(z**(-1.0) + np.sum(h**(-1.0)))\n",
    "        \n",
    "            delta = (g-c)/h\n",
    "            alpha_new -= delta\n",
    "        \n",
    "            if np.any(alpha_new) < 0:\n",
    "                alpha_new = self.newton_raphson(alpha_old/10, gamma_matrix)\n",
    "                return alpha_new\n",
    "        \n",
    "            if (t > 1) & self._convergence_(delta, np.zeros((1,k))):\n",
    "                break\n",
    "            else:\n",
    "                alpha_old = alpha_new.copy()\n",
    "            \n",
    "        return alpha_new\n",
    "    \n",
    "    \n",
    "    @numba.jit(cache = True)\n",
    "    def E(self, doc, alpha_old, beta_old, beta_new, gamma_matrix, N_d, M):\n",
    "        for i in range(M):\n",
    "            gamma, Phi = self.Estep(doc[i], alpha_old, beta_old, N_d[i])\n",
    "            beta_new = self._accumulate_Phi(beta_new, Phi, doc[i])\n",
    "            gamma_matrix[i,:] = gamma\n",
    "        return beta_new, gamma_matrix\n",
    "    \n",
    "\n",
    "    def fit(self, doc, vocabulary):\n",
    "        '''\n",
    "        Latent Dirichlet Allocation Model.\n",
    "        doc is a set of documents, each document is a dictionary.\n",
    "        vocabulary contains the words in all documents.\n",
    "        Return updated alpha and beta.\n",
    "        '''\n",
    "            \n",
    "        k = self._k\n",
    "        max_iter = self._max_em_iter\n",
    "            \n",
    "        N_d = [len(d) for d in doc] # Get the length of each document.\n",
    "        V = len(vocabulary) # Get the length of vocabulary\n",
    "        M = len(doc) # Get the document number.\n",
    "    \n",
    "       # Initialize alpha, beta and the statistics od gamma\n",
    "        alpha_new, beta_new = self.initializaiton(V)\n",
    "        gamma_matrix = np.zeros((M, k))\n",
    "    \n",
    "        for iter in range(max_iter):\n",
    "            beta_old = beta_new.copy()\n",
    "            alpha_old = alpha_new.copy()\n",
    "            \n",
    "            # E step\n",
    "            beta_new, gamma_matrix = self.E(doc, alpha_old, beta_old, beta_new, gamma_matrix, N_d, M)\n",
    "        \n",
    "            # M step\n",
    "            alpha_new = self.newton_raphson(alpha_old, gamma_matrix)\n",
    "            beta_new = self._normalization_col(beta_new)\n",
    "        \n",
    "            # check convergence\n",
    "            if self._convergence_(alpha_new, alpha_old) & self._convergence_(np.sum(beta_new,0), np.sum(beta_old,0)):\n",
    "                break\n",
    "        \n",
    "        return alpha_new, beta_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model1 = LDA_test1(k=10) \n",
    "#%timeit alpha, beta = lda_model1.fit(doc, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta = lda_model1.fit(doc, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(alpha_list, topic_num):\n",
    "    return list(map(alpha_list.index, heapq.nlargest(topic_num, alpha_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(words_list, word_num):\n",
    "    '''\n",
    "    Get the words with largetest probilities in a specific topic.\n",
    "    words_list is a list of probability of words under a specific topic.\n",
    "    word_num is the number of words that we return.\n",
    "    Return the index of words in vocabulary.\n",
    "    '''\n",
    "    return list(map(words_list.index, heapq.nlargest(word_num, words_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 0, 8, 5, 7, 9, 4, 1, 3, 2]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_list = list(alpha)\n",
    "get_topic(alpha_list, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_top = get_words(list(beta[:,4]), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_list(index):\n",
    "    \"\"\"transform the top_words into a list\"\"\"\n",
    "    topic = []\n",
    "    words_top = get_words(list(beta[:,index]), 10)\n",
    "    for i in words_top:\n",
    "        topic.append(vocabulary[i])\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = word_to_list(4)\n",
    "list2 = word_to_list(3)\n",
    "list3 = word_to_list(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>President Election</th>\n",
       "      <th>Medical</th>\n",
       "      <th>Astronomy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trump</td>\n",
       "      <td>patient</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>republican</td>\n",
       "      <td>doctor</td>\n",
       "      <td>game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>report</td>\n",
       "      <td>studi</td>\n",
       "      <td>earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elect</td>\n",
       "      <td>pain</td>\n",
       "      <td>speci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>campaign</td>\n",
       "      <td>perform</td>\n",
       "      <td>planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>democrat</td>\n",
       "      <td>team</td>\n",
       "      <td>episod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hous</td>\n",
       "      <td>vaccin</td>\n",
       "      <td>bail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>say</td>\n",
       "      <td>show</td>\n",
       "      <td>mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>accord</td>\n",
       "      <td>pruitt</td>\n",
       "      <td>nasa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>make</td>\n",
       "      <td>life</td>\n",
       "      <td>moon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  President Election  Medical Astronomy\n",
       "0              trump  patient     space\n",
       "1         republican   doctor      game\n",
       "2             report    studi     earth\n",
       "3              elect     pain     speci\n",
       "4           campaign  perform    planet\n",
       "5           democrat     team    episod\n",
       "6               hous   vaccin      bail\n",
       "7                say     show       mar\n",
       "8             accord   pruitt      nasa\n",
       "9               make     life      moon"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'President Election': list1, 'Medical': list2, 'Astronomy': list3}  \n",
    "result = pd.DataFrame(dict) \n",
    "result "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
