{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paper: Latent Dirichlet Allocatio\n",
    "- Author: David M.Blei, Andrew Y.Ng, Michael I.Jordan\n",
    "- Teammates: Yizi Lin, Siqi Fu\n",
    "- Github: https://github.com/lyz1206/lda.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Latent Dirichlet Allocation* (LDA) is a generative probabilitistic model dealing with collections of data such as corpus. Based on the assumption of bag of word and exchangeability, each document in corpus is modeled as random mixture over latent topics and each topic is modeled by a distribution over words. Document is represented in a form of topic probability. In this project, we foucs on the text data, and in this case, each document is represented as a topic probability. We implement *variational inference* and *EM algorithm* to estimate parameters and performed optimazation method to make our algorithm more efficient. We compare LDA with other *topic model* like LSI and HDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*key words*: Topic Model, Latent Dirichlet Allocation, Variational Inference, EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Background\n",
    "In our project, we use the the paper \"Latent Dirichlet Allocation\" by David M. Blei, Andrew Y. Ng and Michael I.Jordan.  \n",
    "\n",
    "Latent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus, it uses a three-level hierarchical Bayesian model to describe the word generative process. Its basic idea is that each document are represented as random mixtures over latent topics, and each topic is characterized by a distribution over words. In general, LDA assumes the following generative process for each document w in a corpus D:  \n",
    "1. Choose $N \\sim Possion(\\xi)$, which represents the document length.\n",
    "2. Choose $\\theta \\sim Dir(\\alpha)$, which $\\theta$ is a column vector representing the topic probability.\n",
    "3. For each of the N words:\n",
    "    - Choose $z_n\\sim Multinomial(\\theta)$, which represents current topic.\n",
    "    - Choose $w_n$ based on $p( w_n \\mid z_n; \\beta)$  \n",
    "\n",
    "There are three critical assumption for this model:  \n",
    "- the dimensionality k of the Dirichlet distribution is assumed known and fixed.\n",
    "- $\\beta$ is a V $\\times$ k matrix, where $\\beta_{ij} = P( w^j = 1 \\mid z^i = 1)$, which means $\\beta$ represents the probability of generating one particular word given the particular topic. $\\beta$ is also assumed to be known and fix.\n",
    "- words are generated by topics and those topics are infinitely exchangeable within a document.\n",
    "\n",
    "The generating process is represented as a probabilistic graphical model below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Figures/figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the model described above, the joint distribution of a topic mixture $\\theta$, a set of N topics z, and a set of N words w is given by:  \n",
    "$$\n",
    "p(\\theta,z,w|\\alpha, \\beta)=p(\\theta|\\alpha)\\prod_{n=1}^{N}p(z_n|\\theta)p(w_n|z_n, \\beta)\n",
    "$$  \n",
    "\n",
    "Integrating over $\\theta$ and summing over z, we obtain the marginal distribution of a document:  \n",
    "$$\n",
    "p(w|\\alpha, \\beta) = \\int p(\\theta|\\alpha)(\\prod_{n=1}^{N}\\sum_{z_n}p(z_n|\\theta)p(w_n|z_n,\\beta))d\\theta\n",
    "$$  \n",
    "\n",
    "Finally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus: $$\n",
    "P(D \\mid \\alpha, \\beta) = \\prod_{d = 1}^{M} \\int p(\\theta_d \\mid \\alpha)(\\prod_{n = 1}^{N_d}\\sum_{z_{dn}}p(z_{dn}\\mid \\theta_d)  p(w_{dn} \\mid z_{dn},\\beta)) d \\theta_d\n",
    "$$  \n",
    "\n",
    "Using Bayesian rule, we can get the formula of the posterior distribution of the hidden variables given a document:\n",
    "$$\n",
    "p(\\theta,z|w, \\alpha, \\beta) = \\frac{p(\\theta,z,w|\\alpha,\\beta)}{p(w|\\alpha,\\beta)}\n",
    "$$  \n",
    "\n",
    "However, this distribution is intractable to capture in general. In this paper, the author use variational EM algorithm to approximate the distribution. We will discuss it in Part 3.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, the main goal of LDA is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basis tasks. Common applications involve:\n",
    "- document modeling\n",
    "- text classification\n",
    "- collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a three level hierarchical Bayesian model, LDA is more elaborate than some other latent models such as Mixture of unigrams, and pLSA.  \n",
    "\n",
    "- In the Mixture of unigrams, the word distributions can be viewed as representations od topics under the assumption that each document exihibit only one topic. However, LDA allows documents to exihibit multiple topics to different probabilities. \n",
    "\n",
    "- The pLSA model does solves the problem of Mixture of unigrams, but it has further problems that it is not well-defined generative model of documents,which means that it cannot be used to assign probability to a previously unused document. Also, since the linear growth in parameter of the pLSA model, it can causes overfitting. However, LDA sufferes neigher of those problems.\n",
    "\n",
    "\n",
    "From Mixture of unigrams to PLSA TO lDA, the text modeling is improved step by step. LDA introduces the Dirichlet Distribution in the document to topic layer, which is better than PLSA, so that the number of model parameters does not expand with the increase of corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Description of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 2, we have mentioned that the posterior distribution of the hidden variables is intractable to capture in general, so the authors in this paper use variational EM algorithm to approximate it. Generally, this algorithm follows such iteration:  \n",
    "1. (E-step) For each document, find the optimizing values of the variational parameters{$\\gamma_d^\\ast$, \\Phi_d^\\ast, d \\in D}. \n",
    "2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\alpha$ and $\\beta$. This correspondings to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* E-step \n",
    "\n",
    "The main idea in this step is to find the tightest possible lower bound of the log likelihood and choose variational parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we show the procedure of finding the tightest lower bound of the log likelihood.   \n",
    "\n",
    "We begin by applying `Jensen's inequality` to bound the log likehood of document:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "log \\space p \\space (w \\mid \\alpha, \\beta) &= log \\int \\sum_z p(\\theta,z,w \\mid \\alpha, \\beta ) d \\theta\\\\\n",
    "&=log \\int \\sum_z \\frac{p(\\theta,z,w \\mid \\alpha, \\beta)\\space q(\\theta,z)}{q(\\theta,z)} d\\theta\\\\\n",
    "&\\ge \\int \\sum_z q(\\theta,z) \\space log \\space p(\\theta,z,w \\mid \\alpha,\\beta) d\\theta - \\int \\sum_z q(\\theta,z) \\space log \\space q(\\theta,z) d\\theta\\\\\n",
    "&= E_q[log \\space p(\\theta, z,w \\mid \\alpha, \\beta)] -E_q[log\\space q(\\theta, z)]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Form the above equation, we get a lower bound of the likelihood for variational distribution $q(\\theta,z \\mid \\gamma,\\phi)$.\n",
    "  \n",
    "The difference between the left side and right side of the above qeuation represents the `KL` divergence between variational posterior probability and the true posterior probability. Let $L(\\gamma,\\phi: \\alpha, \\beta)$ denote the right-hand side, and we can get:\n",
    "  \n",
    "$$log p(w \\mid \\alpha, \\beta) = L (\\gamma, \\phi :\\alpha,\\beta) + D(q(\\theta, z \\mid \\gamma, \\phi) \\mid \\mid p(\\theta,z \\mid w, \\alpha,\\beta))$$\n",
    "  \n",
    "This shows the maximize lower bound $L(\\gamma, \\phi :\\alpha,\\beta)$ with respect to $\\gamma$ and $\\phi$ is equivalent to minimizing the KL divergence.   \n",
    "\n",
    "So we successfully translate the into the optimization problem as below:\n",
    "  \n",
    "$$(\\gamma^*,\\phi^*) = argmin_{\\gamma,\\phi} D(q(\\theta,z \\mid \\gamma, \\phi) \\mid \\mid p (\\theta,z \\mid w,\\alpha,\\beta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we obtain a tractable family of the $q(\\theta,z)$.  \n",
    "\n",
    "In the paper, the authors drop the edges between $\\theta$, z and w, as well as the w nodes. This procedure is shown below.  \n",
    "\n",
    "![image](Figures/figure2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $q(\\theta,z)$ is characterized by the following variational distribution:  \n",
    "$$\n",
    "q(\\theta,z|\\gamma, \\Phi) = q(\\theta|\\gamma)\\prod_{n=1}^{N}q(z_n|\\Phi_n) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thirdly, we expand the lower bound using the factorizations of p and q:\n",
    "$$\n",
    "L(\\gamma,\\phi: \\alpha,\\beta) = E_q[log p(\\theta \\alpha)] +E_q [log p(z \\mid \\theta )] +E_q [log p(w \\mid z,\\beta)] -E_q[log q(\\theta)] -E_q[log q(z)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use Lagrange method to maximize the lower bound with respect to the variational parameters $\\Phi$ and $\\gamma$. The updated equations are:  \n",
    "$$\\phi_{ni} \\propto \\beta_{iw_n} exp[E_q (log(\\theta_i) \\mid \\gamma)]$$\n",
    "$$\\gamma_i = \\alpha_i +\\sum_{n = 1}^N \\phi_{ni}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudocode of E-step is as follow:  \n",
    "![image](Figures/figure3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- M-step   \n",
    "\n",
    "The main in M-step is to maximize the resulting lower bound on the log likelohood with respect to the model parameters $\\alpha$ and $\\beta$.  \n",
    "\n",
    "We update $\\beta$ through Lagrange method.\n",
    "$$\n",
    "L_{[\\beta]} = \\sum_{d=1}^{M}\\sum_{n=1}^{N_d}\\sum_{i=1}^{k}\\sum_{j=1}^{V}\\Phi_{dni}w_{dn}^jlog\\beta_{ij}+\\sum_{i=1}^{k}\\lambda_i(\\sum_{j=1}^{V}\\beta_{ij}-1)\n",
    "$$\n",
    "So the update equation is:\n",
    "$$\n",
    "\\beta_{ij} \\propto \\sum_{d=1}^{M}\\sum_{n=1}^{N_d}\\Phi_{dn_i}w^{j}_{dn}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ is updated by Newton-Raphson Method:\n",
    "$$\\alpha_{new} = \\alpha_{old} -H (\\alpha_{old})^{-1} g(\\alpha_{old})$$\n",
    "where $H(\\alpha)$ is the Hessian matrix and $g(\\alpha)$ is the gradient at point $\\alpha$. This algorithm scales as $O(N^3)$ due to the matrix inversion.  \n",
    "Instead, if the Hessian matrix has a special struture $H = diag(h) + \\textbf{1} z \\textbf{1}^T$, we are able to yields a Newton-Raphson algorithm that has linear complexity.  This precesure is shown below:\n",
    "$$\n",
    "H^{-1} = diag(h)^{-1} - \\frac{diag(h)^{-1} \\textbf{1} \\textbf{1}^T diag(h)^{-1}}{z^{-1} + \\sum_{j = 1}^k h_j^{-1}}\\\\\n",
    "$$  \n",
    "\n",
    "Multiplying by the gradient, we can get the ith component as:\n",
    "$$(H^{-1} g)_i = \\frac{g_i-c}{h_i}$$\n",
    "where $c = \\frac{\\sum_{j=1}^k g_j/h_j}{z^{-1} +\\sum_{j = 1}^{k} j_j^{-1}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we use variational EM algorithm for LDA model to find the value of parameter $\\alpha$ and $\\beta$, to maximize the marginal log likelihood. In general, there are two parts that needed to be optimized.  \n",
    "1. E-step: For each document d, calculate the optimizing values of the variational parameters : $\\gamma_d^\\ast$ and $Phi_d^\\ast$.  \n",
    "2. M-step: Based on the results of E-step, update $\\alpha$ and $\\beta$.  \n",
    "\n",
    "In the previous part(the plain version), we have optimized the M-step. More specifically, we update $\\alpha$ through the Newton-Raphson methods for a Hessian with special structure, which is mentioned in the paper and decrease the time complexity from $O(N^3)$ to linearity.    \n",
    "  \n",
    "  \n",
    "In this part, our main goal is to optimize the E-step. There are two processes here: optimize $\\gamma_d^\\ast$ and $Phi_d^\\ast$, then calculate the statistics for M-step. \n",
    "\n",
    "Method we use:\n",
    "- Vectorization:  in `Estep_singedoc()` function, usd matrix to avoid the use of for loop.\n",
    "- JIT compilation: for `accumulate_Phi()` and `Estep()` function.\n",
    "\n",
    "We also tried the cython method, but unfortunately it didn't improve the code performance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numba\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import PorterStemmer\n",
    "from scipy.special import digamma, polygamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Estep_original(doc, alpha, beta, k, N_d, max_iter = 50):\n",
    "    '''\n",
    "    E step for a document, which calculate the posterior parameters.\n",
    "    beta and alpha is coming from previous iteration.\n",
    "    Return Phi and gamma  of a document.\n",
    "    '''\n",
    "    gamma_old = [alpha[i] + N_d/k  for i in range(k)] \n",
    "    row_index = list(doc.keys())\n",
    "    word_count = np.array(list(doc.values()))\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        # Update Phi\n",
    "        Phi = np.zeros((N_d, k))\n",
    "        for i in range(N_d):\n",
    "            for j in range(k):\n",
    "                Phi[i,j] = beta[row_index[i],j]*np.exp(digamma(gamma_old[j]))\n",
    "            Phi[i,:] = Phi[i,:]/np.sum(Phi[i,:])\n",
    "         \n",
    "        \n",
    "        \n",
    "        # Update gamma\n",
    "        Phi_sum = np.zeros(k)\n",
    "        for j in range(k):\n",
    "            z = 0\n",
    "            for i in range(N_d):\n",
    "                z += Phi[i,j] * word_count[i]\n",
    "            Phi_sum[j] = z\n",
    "        \n",
    "        gamma_new = alpha + Phi_sum\n",
    "                \n",
    "        # converge or not\n",
    "        if(i>0) and (convergence(gamma_new, gamma_old)):\n",
    "            break\n",
    "        else:\n",
    "            gamma_old = gamma_new.copy()\n",
    "    \n",
    "    return gamma_new, Phi  \n",
    "\n",
    "def accumulate_Phi_original(beta, Phi, doc):\n",
    "    '''\n",
    "    This function accumulates the effect of Phi_new from all documents after e step.\n",
    "    beta is V*k matrix.\n",
    "    Phi is N_d * k matrix.\n",
    "    Return updated beta.\n",
    "    '''\n",
    "    \n",
    "    row_index = list(doc.keys())\n",
    "    word_count = list(doc.values())\n",
    "    for i in range(len(row_index)):\n",
    "        beta[row_index[i],:] = word_count[i] * Phi[i,:]\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opitimized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Estep_singedoc(doc, alpha, beta, k, N_d, max_iter = 50):\n",
    "    '''\n",
    "    E step for a document, which calculate the posterior parameters.\n",
    "    beta and alpha is coming from previous iteration.\n",
    "    Return Phi and gamma  of a document.\n",
    "    '''\n",
    "        \n",
    "    gamma_old = alpha + np.ones(k) * N_d/k\n",
    "    row_index = list(doc.keys())\n",
    "    word_count = np.array(list(doc.values()))\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Update Phi\n",
    "        Phi_exp = np.exp(digamma(gamma_old))\n",
    "        Phi = beta[row_index,:] @ np.diag(Phi_exp)\n",
    "        Phi_new = normalization_row(Phi)\n",
    "        \n",
    "        # Update gamma\n",
    "        Phi_sum = Phi_new.T @ word_count[:,None] # k-dim\n",
    "        gamma_new = alpha + Phi_sum.T[0]\n",
    "        \n",
    "        # Converge or not\n",
    "        if (i>0) & convergence(gamma_new, gamma_old):\n",
    "            break\n",
    "        else:\n",
    "            gamma_old = gamma_new.copy()\n",
    "    \n",
    "            \n",
    "    return gamma_new, Phi_new\n",
    "\n",
    "\n",
    "@numba.jit(cache = True)\n",
    "def accumulate_Phi(beta, Phi, doc):\n",
    "    '''\n",
    "    This function accumulates the effect of Phi_new from all documents after e step.\n",
    "    beta is V*k matrix.\n",
    "    Phi is N_d * k matrix.\n",
    "    Return updated beta.\n",
    "    '''       \n",
    "    beta[list(doc.keys()),:] += np.diag(list(doc.values())) @ Phi\n",
    "    \n",
    "    return beta\n",
    "\n",
    "\n",
    "@numba.jit(cache = True)\n",
    "def Estep(doc, alpha_old, beta_old, beta_new, gamma_matrix, k, N_d, M):\n",
    "    '''\n",
    "    Calculate $\\gamma$ and $\\Phi$ for all documents.\n",
    "    '''\n",
    "    for i in range(M):\n",
    "        gamma, Phi = Estep_singedoc(doc[i], alpha_old, beta_old, k, N_d[i])\n",
    "        beta_new = accumulate_Phi(beta_new, Phi, doc[i])\n",
    "        gamma_matrix[i,:] = gamma\n",
    "    return beta_new, gamma_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some helpful functions for comparison\n",
    "def convergence(new, old, epsilon = 1.0e-3):\n",
    "    '''\n",
    "    Check convergence.\n",
    "    '''\n",
    "    return np.all(np.abs(new - old)) < epsilon\n",
    "\n",
    "def normalization_row(x):\n",
    "    '''\n",
    "    Normaize a matrix by row.\n",
    "    '''\n",
    "    return x/np.sum(x,1)[:,None]\n",
    "\n",
    "def initializaiton(k, V):\n",
    "    '''\n",
    "    Initialize alpha and beta. \n",
    "    alpha is a k-dim vector. beta is V*k matrix.\n",
    "    '''\n",
    "    np.random.seed(12345)\n",
    "    alpha = np.random.uniform(size = k)\n",
    "    \n",
    "    alpha_output = alpha/np.sum(alpha)\n",
    "    \n",
    "    beta_output = np.random.dirichlet(alpha_output, V)\n",
    "        \n",
    "    return alpha_output, beta_output\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    '''\n",
    "    Lenmmatize and stem the text.\n",
    "    '''\n",
    "    return PorterStemmer().stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    '''\n",
    "    Preprocess the text.\n",
    "    '''\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data and preprocess it.\n",
    "\n",
    "data = pd.read_csv('data/articles.csv', error_bad_lines=False)\n",
    "document = data[['content']].iloc[:50,:].copy()\n",
    "document['index'] = document.index\n",
    "processed_docs = document['content'].map(preprocess)\n",
    "vocabulary = gensim.corpora.Dictionary(processed_docs)\n",
    "#vocabulary.filter_extremes(no_below=5, no_above=0.1, keep_n=100)\n",
    "bow_corpus = [vocabulary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "doc = [dict(bow) for bow in bow_corpus]\n",
    "\n",
    "N_d = [len(d) for d in doc]\n",
    "V = len(vocabulary)\n",
    "M = len(doc)\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.2 s ± 2.2 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "alpha0, beta0 = initializaiton(k, V)\n",
    "beta = beta0\n",
    "gamma_matrix_origin = np.zeros((M, k))\n",
    "for i in range(M):\n",
    "    gamma, Phi = Estep_original(doc[i], alpha0, beta0, k, N_d[i])\n",
    "    beta = accumulate_Phi_original(beta, Phi, doc[i])\n",
    "    gamma_matrix_origin[i,:] = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimized Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 ms ± 10.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "alpha0, beta0 = initializaiton(k, V)\n",
    "beta = beta0\n",
    "gamma_matrix_opt = np.zeros((M, k))\n",
    "beta_new, gamma_matrix_opt = Estep(doc, alpha0, beta0, beta, gamma_matrix_opt, k, N_d, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclusion  \n",
    "We use 50 documents here, and set up k=3. After opitimizaion, the running time decreased from 15.2 to 221 ms. the increase percentage is nearly 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Applications to simulated data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we use our model on a simulated data set.  Given $\\alpha$, $\\beta$, k, M, V, and N_d by ourselves, the data process is described in Part 2.  The corresponding code and result is in the file Test-Simulated.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real value of $\\alpha$ is [0.15, 0.35, 0.5]. The results of the simulated shows that, , the $\\hat{\\alpha}$ is [0.49, 0.34,  0.16]. We find that the estiamted value is able to approximate the true value. \n",
    "\n",
    "We also calculate the average difference between $\\beta$ and $\\hat{\\beta}$, which is equals to 0.0059 and is acceptable.  \n",
    "\n",
    "In conclusion, the LDA method is able to capture the true value of $\\alpha$ and $\\beta$ if we run the code for enough time and start with a \"good\" point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 6: Real data set  \n",
    "\n",
    "In this part, we implent our algorithm on two real dataset.  \n",
    "We preprocess the dataset before using them. Operation includes: spliting the sentences into words, lemmatizating, removing stop words, creating vocaubulary and establish corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset in Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, the authors used 16,000 documents from a subset of the TREC AP corpors(Harman, 1992). It is not easy to get the TREC datast since we need to sign an individual agreement and ask for approval from NIST. Instead, we download the sample data on [Blei's webpage](http://www.cs.columbia.edu/~blei/lda-c/). This sample is just a subset of the data that the authors used in the paper, so we cannot get the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Figures/example1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic words from some of the resulting multinomial distribution $p(w|z)$ are illustrated above. These distribution seem to capture some hidden topics in the corpus.  \n",
    "For example, \"millison\", \"market\", \"stock\", \"company\" are common words in the topic like \"economy\", and \"president\", \"reagan\", \"statement\" and \"troop\" are common words in the topic like \"politics\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is named \"All the news\" and it is coming from [kaggle](https://www.kaggle.com/snapcrack/all-the-news). The dataset contains articles from New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News and so on. The original dataset has three csv file, but we just use the first 1000 rows in the second file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Figures/example 2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous dataset, the LDA model captures some hidden topics in the corpus. For example, words like \"space\", \"planet\", \"earth\" and \"universe\" are common in astronomy area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, our package works well. The LDA model is able to capture the hidden topic in the corpus and to provide reasonable insights to us, which is useful for text classification and collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7 Comparative Analysis with Competing Algorihtms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we compare the LDA method with two competing algorithm: Latent Semantic Indexing (LSI) and Hierarchical Dirichlet process(HDP). We still use the \"All the news\" dataset to evaluate the performance of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/articles.csv')\n",
    "document = df[['content']].copy()\n",
    "document['index'] = document.index\n",
    "processed_docs = document['content'].map(preprocess)\n",
    "vocabulary = gensim.corpora.Dictionary(processed_docs)\n",
    "bow_corpus = [vocabulary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA vs LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914 ms ± 170 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit LsiModel(bow_corpus, 30, id2word = vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.33 s ± 144 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit LdaModel(bow_corpus, 30, id2word = vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsamodel = LsiModel(bow_corpus, 30, id2word = vocabulary)\n",
    "ldamodel = LdaModel(bow_corpus, 30, id2word = vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.017*\"trump\" + 0.008*\"say\" + 0.007*\"presid\" + 0.006*\"like\" + 0.005*\"peopl\" + 0.004*\"state\" + 0.004*\"american\" + 0.004*\"time\" + 0.004*\"republican\" + 0.003*\"year\"\n",
      "Topic: 1 \n",
      "Words: 0.017*\"trump\" + 0.009*\"presid\" + 0.006*\"say\" + 0.004*\"like\" + 0.004*\"american\" + 0.004*\"work\" + 0.004*\"peopl\" + 0.004*\"time\" + 0.004*\"year\" + 0.003*\"go\"\n",
      "Topic: 2 \n",
      "Words: 0.015*\"trump\" + 0.008*\"school\" + 0.007*\"say\" + 0.006*\"like\" + 0.005*\"state\" + 0.005*\"presid\" + 0.005*\"year\" + 0.005*\"peopl\" + 0.004*\"work\" + 0.003*\"countri\"\n",
      "Topic: 3 \n",
      "Words: 0.011*\"trump\" + 0.008*\"say\" + 0.007*\"peopl\" + 0.006*\"like\" + 0.005*\"year\" + 0.004*\"presid\" + 0.004*\"think\" + 0.004*\"work\" + 0.003*\"know\" + 0.003*\"state\"\n",
      "Topic: 4 \n",
      "Words: 0.006*\"say\" + 0.005*\"peopl\" + 0.005*\"work\" + 0.005*\"presid\" + 0.004*\"trump\" + 0.004*\"like\" + 0.004*\"think\" + 0.004*\"time\" + 0.003*\"go\" + 0.003*\"come\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    if idx<5:\n",
    "        print('Topic: {} \\nWords: {}'.format(idx, topic))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.767*\"trump\" + 0.307*\"presid\" + 0.128*\"busi\" + 0.116*\"organ\" + 0.103*\"compani\" + 0.086*\"hotel\" + 0.084*\"properti\" + 0.079*\"conflict\" + 0.074*\"accord\" + 0.073*\"elect\"\n",
      "Topic: 1 \n",
      "Words: 0.367*\"say\" + 0.270*\"peopl\" + -0.213*\"trump\" + 0.206*\"like\" + 0.153*\"year\" + 0.152*\"state\" + 0.150*\"work\" + 0.136*\"think\" + 0.128*\"school\" + 0.127*\"time\"\n",
      "Topic: 2 \n",
      "Words: -0.293*\"senat\" + -0.232*\"republican\" + -0.190*\"democrat\" + -0.171*\"state\" + -0.166*\"trump\" + -0.162*\"confirm\" + 0.161*\"peopl\" + -0.160*\"vote\" + -0.136*\"hous\" + -0.135*\"nomin\"\n",
      "Topic: 3 \n",
      "Words: 0.667*\"school\" + 0.409*\"student\" + 0.253*\"educ\" + 0.152*\"public\" + 0.122*\"univers\" + -0.118*\"peopl\" + 0.111*\"devo\" + 0.102*\"state\" + 0.098*\"teacher\" + 0.092*\"charter\"\n",
      "Topic: 4 \n",
      "Words: -0.253*\"patient\" + 0.246*\"state\" + -0.208*\"senat\" + -0.193*\"drug\" + 0.169*\"countri\" + -0.143*\"confirm\" + 0.142*\"american\" + -0.141*\"studi\" + -0.140*\"say\" + 0.140*\"unit\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lsamodel.print_topics(-1):\n",
    "    if idx<5:\n",
    "        print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows that the LSI algorithm is even faster, since it implement a SVD decompisition to reduce the dimension of the input. Howvever, the LDA method implement a variational EM algorithm or gibbs sampling, which require a lot of iteration to make the estimated value of every document converge, and thus is time consuming. \n",
    "As for the result, we find that all the coefficients in LDA is positive, but some of the coefficients in LSI is negative. \n",
    "In conclusion, the speed of LSI algotithm is significantly faster than the LDA algorithm. In the case that we need to use cross-validation to choose the topic number, LSI is more effective. However, the components of the topic in LSI method are arbitrarily positive/negative, which it is difficult to interpret. Moreover, LSI is unable to capture the multiple meanings of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA vs HDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One character of the LDA algorithm is that we need to specify the number of topic. However, in most cases, we do not know the exactly topic numbers. Cross validatio is a method to deal with this problem. However, in the previous part, we have shown that the LDA algorithm is less effective, using cross validation is time consuming.  \n",
    "HDP algorithm is a natural nonparametric generalization of Latent Dirichlet allocation, where the number of topics can be unbounded and learnt from data, so we don't need to select the topic numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.97 s ± 897 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit HdpModel(bow_corpus, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.23 s ± 370 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit LdaModel(bow_corpus, 30, id2word = vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdp = HdpModel(bow_corpus, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.022*trump + 0.009*presid + 0.007*say + 0.005*like + 0.004*state + 0.004*peopl + 0.004*time + 0.003*year + 0.003*countri + 0.003*busi\n",
      "Topic: 1 \n",
      "Words: 0.010*trump + 0.007*say + 0.006*peopl + 0.006*like + 0.005*state + 0.004*year + 0.004*presid + 0.004*time + 0.004*work + 0.003*american\n",
      "Topic: 2 \n",
      "Words: 0.006*say + 0.004*like + 0.004*patient + 0.004*peopl + 0.003*trump + 0.003*studi + 0.003*work + 0.003*year + 0.003*time + 0.003*doctor\n",
      "Topic: 3 \n",
      "Words: 0.010*trump + 0.005*say + 0.004*presid + 0.003*like + 0.003*state + 0.003*polici + 0.002*year + 0.002*american + 0.002*school + 0.002*order\n",
      "Topic: 4 \n",
      "Words: 0.017*trump + 0.003*busi + 0.003*presid + 0.003*countri + 0.002*financi + 0.002*like + 0.002*say + 0.002*organ + 0.002*elect + 0.002*state\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in hdp.print_topics(-1):\n",
    "    if idx<5:\n",
    "        print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the speed the LDA algorithm is faster, if we want to implement cross validaiton to ensure the topic numbers, we have to run the model serveal time, and thus the total time is longer than the HDP algorithm. In this case, HDP is better.  \n",
    "However, compared the results from two algorithm, it is obvious that they are different. Sometimes it is difficult to interpret the result of HDP algorithm. What's more, if the previous experience tells us what the topic number is, the LDA model is more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 8  Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the performance of our algorithm on the real dataset and the dataset in paper, our algorithm does fulfill the need that divide the document into different topics and explore the words occurring in that topic of different weight.<br>\n",
    "\n",
    "LDA can be used in a variety of purposes:  \n",
    "\n",
    "- Clustering: The topic in the clustering center, and the document associate with multer clusters. Clustering is very helpful in organizing and summarizing article collections.\n",
    "- Feature generation: LDA can generate features for other machine learning algorithm. As mentioned before, LDA generates topics for each document, and these K topics can be treated as K features, and these features can be used to predict as in logistic regression and decision tree.\n",
    "- Dimension Reduction: LDA provides a topic distribution which can be seen as a concise\n",
    " summary of the article.Comparing articles in this dimensionally reduced feature space is more meaningful than in the feature space of the original vocabulary.\n",
    "\n",
    "Even though the LDA performs well in our dataset, it does have some limitations. For example, the number of topics $k$ is fixed and must be known ahead of time. Also, Since the Dirichlet topic distribution cannot capture correlations, the LDA can only capture uncorrelated topics. Finally, the LDA algorithm is based on the assumption of BoW, which assumes words are exchangeable, and does not consider sentence sequence. <br>  \n",
    "\n",
    "To overcome the limitation, extend LDA to the distributions on the topic variables are elaborated. For example, arranging the topics in a time series, so that it relax the full exchangeability assumption to one of partial exchangeability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 9 References/bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" the Journal of machine Learning research 3 (2003): 993-1022."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
